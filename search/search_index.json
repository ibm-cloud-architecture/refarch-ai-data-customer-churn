{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Customer Churn - An example of Reference Implementation of Data & AI The goals of this implementation is to illustrate how to build an intelligent application using: Data Ingestion and Organization Model Development and Deployment Batch Scoring Model MOnitoring Cognitive service like Tone Analyzer and Chatbot Integrate with Customer Data base and unstructured data to build a customer churn predictive scoring model Deploy and monitor the model This project is one example of how to apply the data and AI reference architecture as presented here . Modern applications are leveraging a set of capabilities to do a better assessment of customer characteristics and deliver the best actions or recommendations. The technologies involved, include artificial intelligence, data governance, ingestion, enrichment, storage, analysis, machine learning, unstructured data classifications, natural language understanding, tone analysis, and hybrid integration.... Update 09/17/19 Target audiences IT Architects who want to understand the components involved and the architecture constraints and design considerations Developers who want to get starting code, and educate themselves on the related technologies Data Scientists who want to complement machine learning with cognitive output like classification Key points Data scientists need different source of data, structured from traditional SQL based database (e.g. the customers and accounts data) and unstructured output of new cognitive services. Data Scientists work hand by hand with application developers to quickly deliver solution to the business. Data access layer to traditional relational data should be done with a micro service approach exposing RESTful API or using modern z OS Connect application. Cloud native apps, microservices, can be deployed in public or private cloud, like IBM Cloud private or IKS based on Kubernetes and containers. Public services like the Watson services can be easily integrated within the solution: Watson Data Platform, Tone Analyzer, Watson Assistant. API management is used to present a unique API for customer data, standardize for consumers like the webapp, even if the back end is Java based or z Connected based. Product recommendations may be added to the solution to support business decision from a chatbot conversation taking into account the churn scoring risk. See this note to explain how to leverage IBM Operational Decision Management for that. The high level view of the solution The following figure presents the capabilities used","title":"Customer Churn - An example of Reference Implementation of Data & AI"},{"location":"#customer-churn-an-example-of-reference-implementation-of-data-ai","text":"The goals of this implementation is to illustrate how to build an intelligent application using: Data Ingestion and Organization Model Development and Deployment Batch Scoring Model MOnitoring Cognitive service like Tone Analyzer and Chatbot Integrate with Customer Data base and unstructured data to build a customer churn predictive scoring model Deploy and monitor the model This project is one example of how to apply the data and AI reference architecture as presented here . Modern applications are leveraging a set of capabilities to do a better assessment of customer characteristics and deliver the best actions or recommendations. The technologies involved, include artificial intelligence, data governance, ingestion, enrichment, storage, analysis, machine learning, unstructured data classifications, natural language understanding, tone analysis, and hybrid integration.... Update 09/17/19","title":"Customer Churn - An example of Reference Implementation of Data &amp; AI"},{"location":"#target-audiences","text":"IT Architects who want to understand the components involved and the architecture constraints and design considerations Developers who want to get starting code, and educate themselves on the related technologies Data Scientists who want to complement machine learning with cognitive output like classification","title":"Target audiences"},{"location":"#key-points","text":"Data scientists need different source of data, structured from traditional SQL based database (e.g. the customers and accounts data) and unstructured output of new cognitive services. Data Scientists work hand by hand with application developers to quickly deliver solution to the business. Data access layer to traditional relational data should be done with a micro service approach exposing RESTful API or using modern z OS Connect application. Cloud native apps, microservices, can be deployed in public or private cloud, like IBM Cloud private or IKS based on Kubernetes and containers. Public services like the Watson services can be easily integrated within the solution: Watson Data Platform, Tone Analyzer, Watson Assistant. API management is used to present a unique API for customer data, standardize for consumers like the webapp, even if the back end is Java based or z Connected based. Product recommendations may be added to the solution to support business decision from a chatbot conversation taking into account the churn scoring risk. See this note to explain how to leverage IBM Operational Decision Management for that.","title":"Key points"},{"location":"#the-high-level-view-of-the-solution","text":"The following figure presents the capabilities used","title":"The high level view of the solution"},{"location":"businessproblem/","text":"Story of an unhappy customer Eddy Eddie is an existing Green Telco Inc customer living in Orlando Fl. He has been using the services provided by the Green Telco for the last 2 years. Eddie is traveling to Europe and last week signed up for a Data plan which he is told will double the original data speed. Eddie also bought a phone by paying cash with the understanding that he will get another phone free as part of a promotional plan. Eddie comes back from Europe trip after 5 days. He logs on the Portal to check if the second phone is shipped or not. The portal shows his claim for the same was denied. Eddie is already unhappy that the data plan that he was prescribed was not working in Europe and Asia and on top of this his claim for 2 nd phone was denied. Eddie logs on to the Portal and a chatbot starts a conversation. The Goal is to how to use Data and AI to stop churn of Eddie and make him happy customer again.","title":"Business problem"},{"location":"businessproblem/#story-of-an-unhappy-customer-eddy","text":"Eddie is an existing Green Telco Inc customer living in Orlando Fl. He has been using the services provided by the Green Telco for the last 2 years. Eddie is traveling to Europe and last week signed up for a Data plan which he is told will double the original data speed. Eddie also bought a phone by paying cash with the understanding that he will get another phone free as part of a promotional plan. Eddie comes back from Europe trip after 5 days. He logs on the Portal to check if the second phone is shipped or not. The portal shows his claim for the same was denied. Eddie is already unhappy that the data plan that he was prescribed was not working in Europe and Asia and on top of this his claim for 2 nd phone was denied. Eddie logs on to the Portal and a chatbot starts a conversation. The Goal is to how to use Data and AI to stop churn of Eddie and make him happy customer again.","title":"Story of an unhappy customer Eddy"},{"location":"solution/","text":"Step 0 - Enterprise Applications persist data in Data Stores. They all pushes data to Event Stores for real time processing. Step 1 - Modele Builder build a Customer Churn Model based on Monthly/Haif Yearly data across all Customers. Also deploys the same in a Deployment Infrastructure Step 2 - The Churn Scorer, on weekly basis gets data of all Customers from the Data Store, scores the Customers for their potential to Churn. Also gets the Explanation of the Prediction, Bias of the Model and Qoverall uality/Accuracy of the model from the MOdel Monitoring system. Stores all these information in Data Store. Step 3 - When Eddie logins into the Web Application (after his bad experience with the Data Plan in Europe this week and latest finding that he has not got the 2 nd phone), the Web Application checks the Data Store for Eddie's Churn potential (along with Explanation, Bias and model's accuracy) based on last week's result. It finds that in overall Eddie is a happy customer. So it decides to hand over Eddie to a Chatbot Step 4 - Chatbot starts communicating with Eddie. As the messages got exchanged the Chatbot sends those messages to Watson Natural Language Understanding for analyzing Tone of teh messages. It find the Tone to be continuously comming as Angry/Not Happy. Because of that the Chatbot triggers On Demand Churn scoring for Eddie by calling Churn Scorer. Step 5 - The Churn Scorer gets the latest Real Time events of Eddie from Event Store. Those events reflects Eddie's current activities (including Dropped Call, Long Distance Call etc.). It uses those latest data (along with past data accrued) to score Eddie and it finds that potential of Eddie to Churn is True. It also gets Explanation, current Bias of the Model and Qoverall uality/Accuracy of the model. It stores these information back to Event Store. Step 6 - The Chatbot keeps on polling Event Store for the results from On Demand Scoring for Eddie. It gets it within couple of seconds and identifies that Eddie now has a high potential for Churn. That result is also supported by Explanation, lack of Bias in the Model and overall Quality of the Model. So Chatbot hands over the call to a human representative. Step 7 - Human representative has now all back ground of Eddie's call, his current potential to Churn and reason. He/She, addreses Eddie's problem quickly, offers him some reqrd/incentive and stops him from cancelling the contract. The potential churn is stopped and Eddie is now happy customer again !!!","title":"Solution"},{"location":"Introduction/readme/","text":"","title":"Readme"},{"location":"lab/readme/","text":"Customer Churn Lab for IBM Cloud Public","title":"IBM Cloud"},{"location":"lab/readme/#customer-churn-lab-for-ibm-cloud-public","text":"","title":"Customer Churn Lab for IBM Cloud Public"},{"location":"lab/1. Getting Started/1. Datasets and Sample Code/","text":"Datasets and Sample Code to be used to develop this Use case All necessary datasets to develop this Use case are available in the Datasets folder at the root of this repository. You have to download them before hand for development of this Use case. Sample Codes are available in Notebooks in the Notebook folder at the root of this repository. You have to download them before hand for development of this Use case. There are 2 subfolders under Notebook folder. One has Notebooks for Cloud Oak for Data, the other has same for Watson Studio in IBM Cloud Public. Sample SPSS Modeler flows are available in SPSSModelerFlow folder at the root of this repository. You have to download them before hand for development of this Use case. There are 2 subfolders under SPSSModelerFlow folder. One has Notebooks for Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. There are also Sample Projects available which contains Datasets, Sample Notebooks and Sample SPSS Modeler Flows. You can upload them as a new project (with From File option) in either Cloud Pak for Data instance or in Watson Studio in IBM Public Cloud. These sample projects are available in Sample Projects folder at the root of this repository. There are 2 subfolders under Sample Projects folder. One has project zip file for Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public.","title":"Datasets and Sample Code to be used to develop this Use case"},{"location":"lab/1. Getting Started/1. Datasets and Sample Code/#datasets-and-sample-code-to-be-used-to-develop-this-use-case","text":"All necessary datasets to develop this Use case are available in the Datasets folder at the root of this repository. You have to download them before hand for development of this Use case. Sample Codes are available in Notebooks in the Notebook folder at the root of this repository. You have to download them before hand for development of this Use case. There are 2 subfolders under Notebook folder. One has Notebooks for Cloud Oak for Data, the other has same for Watson Studio in IBM Cloud Public. Sample SPSS Modeler flows are available in SPSSModelerFlow folder at the root of this repository. You have to download them before hand for development of this Use case. There are 2 subfolders under SPSSModelerFlow folder. One has Notebooks for Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. There are also Sample Projects available which contains Datasets, Sample Notebooks and Sample SPSS Modeler Flows. You can upload them as a new project (with From File option) in either Cloud Pak for Data instance or in Watson Studio in IBM Public Cloud. These sample projects are available in Sample Projects folder at the root of this repository. There are 2 subfolders under Sample Projects folder. One has project zip file for Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public.","title":"Datasets and Sample Code to be used to develop this Use case"},{"location":"lab/1. Getting Started/2. Initial Setups for IBM Cloud Pak for Data/","text":"Initial Setup for Cloud Pak for Data Cloud Pak for Data should be either installed for developing this use case or you have to install the same using the instructions available . Once Cloud Pak for Data is installed, various users need to be created. These users should have been already created with necessary access rights. You should have following information available with you for developing this use case - a. Cloud Pak for Data - URL b. Watson Open Scale add on for Cloud Oak for Data - URL c. JDBC URL, Login id and Password for DB2 Wearhouse internal to Cloud Oak for Data d. Credential of a User Profile (user id/password) that has access right of Administrator, Data Engineer, Business Analyst, Data Scientist, Data Steward and App Developer Use the above information to login to Cloud Oak for Data UI and Watson Open Scale UI to ensure that you can get started. On successful login you will see a welcome page as below Next, you should create a Global Connection for DB2 Wearhouse internal to Cloud Oak for Data using following steps - Click on Connections in the left Navigation menu. Connections allow you to create data connections and browse the data connections that you can use to add data to the enterprise governance catalog, analyze data, virtualize data, and more. Click \u2018Add Connection\u2019. The \u2018Add Connection\u2019 will open a screen as below. Provide all the details. Then you can test the connection using \u201cTest Connection\u201d button at the right bottom of the screen. If the \u201cTest Connection\u201d step is successful \u2018Save\u2019 the same. One can also upload custom driver if the data source is not available in the drop-down list of out-of-the box sources. Next, create a Project to get started. A project is a collection of assets that you use to achieve a particular data analysis goal. Your project assets can include: \u2022 Notebooks \u2022 RStudio files \u2022 Models \u2022 Data assets (local files, data sources, and remote data sets) \u2022 Scripts Login with your UserId. Go to Projects from the left Navigation Menu. Then click on Add Project. This would show you the Create Project screen below. If you have not downloaded already, please download the Sample Project file from the Cloud Oak for Data folder under Sample Projects in the root of this repository. After download rename downloaded file to TelcoChurnPrj .zip. To import project select \u2018From file\u2019. drag and drop or browse for the Zip file you downloaded earlier (TelcoChurnPrj .zip). Click \u2018Create\u2019, then the project is created. It will take you to the \u201cAssets\u201d page of this project. Alternatively, you can also create a blank project by using the first left tab and providing the same name as above. Later on you need to add datasets and Notebooks to this project.","title":"Initial Setup for Cloud Pak for Data"},{"location":"lab/1. Getting Started/2. Initial Setups for IBM Cloud Pak for Data/#initial-setup-for-cloud-pak-for-data","text":"Cloud Pak for Data should be either installed for developing this use case or you have to install the same using the instructions available . Once Cloud Pak for Data is installed, various users need to be created. These users should have been already created with necessary access rights. You should have following information available with you for developing this use case - a. Cloud Pak for Data - URL b. Watson Open Scale add on for Cloud Oak for Data - URL c. JDBC URL, Login id and Password for DB2 Wearhouse internal to Cloud Oak for Data d. Credential of a User Profile (user id/password) that has access right of Administrator, Data Engineer, Business Analyst, Data Scientist, Data Steward and App Developer Use the above information to login to Cloud Oak for Data UI and Watson Open Scale UI to ensure that you can get started. On successful login you will see a welcome page as below Next, you should create a Global Connection for DB2 Wearhouse internal to Cloud Oak for Data using following steps - Click on Connections in the left Navigation menu. Connections allow you to create data connections and browse the data connections that you can use to add data to the enterprise governance catalog, analyze data, virtualize data, and more. Click \u2018Add Connection\u2019. The \u2018Add Connection\u2019 will open a screen as below. Provide all the details. Then you can test the connection using \u201cTest Connection\u201d button at the right bottom of the screen. If the \u201cTest Connection\u201d step is successful \u2018Save\u2019 the same. One can also upload custom driver if the data source is not available in the drop-down list of out-of-the box sources. Next, create a Project to get started. A project is a collection of assets that you use to achieve a particular data analysis goal. Your project assets can include: \u2022 Notebooks \u2022 RStudio files \u2022 Models \u2022 Data assets (local files, data sources, and remote data sets) \u2022 Scripts Login with your UserId. Go to Projects from the left Navigation Menu. Then click on Add Project. This would show you the Create Project screen below. If you have not downloaded already, please download the Sample Project file from the Cloud Oak for Data folder under Sample Projects in the root of this repository. After download rename downloaded file to TelcoChurnPrj .zip. To import project select \u2018From file\u2019. drag and drop or browse for the Zip file you downloaded earlier (TelcoChurnPrj .zip). Click \u2018Create\u2019, then the project is created. It will take you to the \u201cAssets\u201d page of this project. Alternatively, you can also create a blank project by using the first left tab and providing the same name as above. Later on you need to add datasets and Notebooks to this project.","title":"Initial Setup for Cloud Pak for Data"},{"location":"lab/1. Getting Started/3. Initial Setups for IBM Cloud Public/","text":"Initial Setup for IBM Cloud Public","title":"Initial Setup for IBM Cloud Public"},{"location":"lab/1. Getting Started/3. Initial Setups for IBM Cloud Public/#initial-setup-for-ibm-cloud-public","text":"","title":"Initial Setup for IBM Cloud Public"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/readme/","text":"Data Ingestion, Organization and Governance for Model Development and Batch Scoring for Customer Churn","title":"Data Ingestion, Organization and Governance for Model Development and Batch Scoring for Customer Churn"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/readme/#data-ingestion-organization-and-governance-for-model-development-and-batch-scoring-for-customer-churn","text":"","title":"Data Ingestion, Organization and Governance for Model Development and Batch Scoring for Customer Churn"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/1. Data Discovery/","text":"Data Discovery - Check for Relevant Data Assets","title":"Data Discovery - Check for Relevant Data Assets"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/1. Data Discovery/#data-discovery-check-for-relevant-data-assets","text":"","title":"Data Discovery - Check for Relevant Data Assets"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/2. Data Governance - Business Glossary, Policy and Rules/","text":"Data Governance - Creating Business Glossary, Policies and Rules","title":"Data Governance - Creating Business Glossary, Policies and Rules"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/2. Data Governance - Business Glossary, Policy and Rules/#data-governance-creating-business-glossary-policies-and-rules","text":"","title":"Data Governance - Creating Business Glossary, Policies and Rules"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/3. Data Ingestion Through Flat Files/","text":"Data Ingestion from Flat Files to a Project If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all data files available in Datasets folder at the root of this repository. b) Next, click Datasets under Assets. In the resulting page click Add Dataset. c) In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files. Next, add the Datasets to Watson Knowledge Catalog (Not Available right now)","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/3. Data Ingestion Through Flat Files/#data-ingestion-from-flat-files-to-a-project","text":"If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all data files available in Datasets folder at the root of this repository. b) Next, click Datasets under Assets. In the resulting page click Add Dataset. c) In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files. Next, add the Datasets to Watson Knowledge Catalog (Not Available right now)","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/4. Data Ingestion to Staginfg Datastore/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"4. Data Ingestion to Staginfg Datastore"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/5. Data Ingestion from Virtualized Data Source/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"5. Data Ingestion from Virtualized Data Source"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/6. Data Organization using PySpark in Notebook/","text":"Data Organization using PySpark in Jupyter Notebook If you are using Sample Project necessary Notebooks are already ingested in your Project. Otherwise, a) first ensure that you have downloaded all Notebook files available in Notebooks folder at the root of this repository. b) Next, click Notebooks under Assets. In the resulting page click Add Notebook. c) Go to From File tab. Upload the notebook 'Prepare and Save Data For Building a ML Model'. Next, open the Notebook using Spark Environment. Use the highest version of Spark Environment to open the Notebook by clicking three vertical dots at the right of the notebook name). Run all cells of the Notebook following the Instructions in the Notebook. Next, add the resulting Datasets to Watson Knowledge Catalog so that the dataset is available for use by others (Not Available right now)","title":"Data Organization using PySpark in Jupyter Notebook"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/6. Data Organization using PySpark in Notebook/#data-organization-using-pyspark-in-jupyter-notebook","text":"If you are using Sample Project necessary Notebooks are already ingested in your Project. Otherwise, a) first ensure that you have downloaded all Notebook files available in Notebooks folder at the root of this repository. b) Next, click Notebooks under Assets. In the resulting page click Add Notebook. c) Go to From File tab. Upload the notebook 'Prepare and Save Data For Building a ML Model'. Next, open the Notebook using Spark Environment. Use the highest version of Spark Environment to open the Notebook by clicking three vertical dots at the right of the notebook name). Run all cells of the Notebook following the Instructions in the Notebook. Next, add the resulting Datasets to Watson Knowledge Catalog so that the dataset is available for use by others (Not Available right now)","title":"Data Organization using PySpark in Jupyter Notebook"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/7. Data Organization using Refinary/","text":"Data Ingestion from Flat Files to a Project If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, first ensure that you have downloaded all data files available in Datasets folder at the root of this repository. Next, click Datasets under Assets. In the resulting page click Add Dataset. In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/7. Data Organization using Refinary/#data-ingestion-from-flat-files-to-a-project","text":"If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, first ensure that you have downloaded all data files available in Datasets folder at the root of this repository. Next, click Datasets under Assets. In the resulting page click Add Dataset. In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/8. Data Organization using SPSS Modeler Flow/","text":"Data Ingestion from Flat Files to a Project If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, first ensure that you have downloaded all data files available in Datasets folder at the root of this repository. Next, click Datasets under Assets. In the resulting page click Add Dataset. In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/1. IBM Cloud Pak for Data/8. Data Organization using SPSS Modeler Flow/#data-ingestion-from-flat-files-to-a-project","text":"If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, first ensure that you have downloaded all data files available in Datasets folder at the root of this repository. Next, click Datasets under Assets. In the resulting page click Add Dataset. In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/2. IBM Cloud Public/readme/","text":"Labs","title":"Labs"},{"location":"lab/2. Data Ingestion, Organization and Governance for Customer Churn/2. IBM Cloud Public/readme/#labs","text":"","title":"Labs"},{"location":"lab/3. Customer Churn Model Development and Deployment/readme/","text":"Customer Churn Model Development and Deployment","title":"Customer Churn Model Development and Deployment"},{"location":"lab/3. Customer Churn Model Development and Deployment/readme/#customer-churn-model-development-and-deployment","text":"","title":"Customer Churn Model Development and Deployment"},{"location":"lab/3. Customer Churn Model Development and Deployment/1. Model Development/1. Model Development using Auto AI/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"1. Model Development using Auto AI"},{"location":"lab/3. Customer Churn Model Development and Deployment/1. Model Development/2. Model Development using PySpark in Jupyter Notebook/","text":"Model Development using PySpark in Jupyter Notebook Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Create and Save Model Using pyspark\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - You can read the data either from a Database/DataWarehouse or Virtual Datasource or Flat File depending on your previous step for Data Ingestion and Data Organization. Accordingly you run appropriate cells in the notebook While Saving the Model using Model name with a prefix of your name/user-id. This is just to ensure that you are not saving a model with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Development using PySpark in Jupyter Notebook"},{"location":"lab/3. Customer Churn Model Development and Deployment/1. Model Development/2. Model Development using PySpark in Jupyter Notebook/#model-development-using-pyspark-in-jupyter-notebook","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Create and Save Model Using pyspark\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - You can read the data either from a Database/DataWarehouse or Virtual Datasource or Flat File depending on your previous step for Data Ingestion and Data Organization. Accordingly you run appropriate cells in the notebook While Saving the Model using Model name with a prefix of your name/user-id. This is just to ensure that you are not saving a model with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Development using PySpark in Jupyter Notebook"},{"location":"lab/3. Customer Churn Model Development and Deployment/1. Model Development/3. Model Development using SPSS Modeler Flow/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"3. Model Development using SPSS Modeler Flow"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/1. Model Deployment in MMD/","text":"Model Deployment in MMD of Cloud Pak for Data Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Deployment in MMD of Cloud Pak for Data"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/1. Model Deployment in MMD/#model-deployment-in-mmd-of-cloud-pak-for-data","text":"Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Deployment in MMD of Cloud Pak for Data"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/2. Model Deployment in WML using API/","text":"Model Deployment using WML API Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/2. Model Deployment in WML using API/#model-deployment-using-wml-api","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/3. Model Deployed in MMD deployed again in WML using API/","text":"Model Deployed in MMD further deployed as Python Function in WML This is to showcase the situation where Model deployed in any arbitrary environment (say Amazon Sage Maker, Azure Ml, Custom Environment, IBM WML in Cloud, etc.) needs to be further deployed in WML in Cloud Pak for Data. This is needed for Watson Open Scale being able to monitor models deployed in any arbitrary deployment environment for AI models. Here we are using MMD as example of that arbitrary Model deployment environment. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018wml-func\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While saving and deploying the Python Function use a name with a prefix of your name/user-id. This is just to ensure that you are not saving the same with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. While creating the Python Function use the scoring URL and access token you got when you deployed the MMD model in one of the previous steps.","title":"Model Deployed in MMD further deployed as Python Function in WML"},{"location":"lab/3. Customer Churn Model Development and Deployment/2. Model Deployment and Release in IBM Cloud Pak for Data/3. Model Deployed in MMD deployed again in WML using API/#model-deployed-in-mmd-further-deployed-as-python-function-in-wml","text":"This is to showcase the situation where Model deployed in any arbitrary environment (say Amazon Sage Maker, Azure Ml, Custom Environment, IBM WML in Cloud, etc.) needs to be further deployed in WML in Cloud Pak for Data. This is needed for Watson Open Scale being able to monitor models deployed in any arbitrary deployment environment for AI models. Here we are using MMD as example of that arbitrary Model deployment environment. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018wml-func\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While saving and deploying the Python Function use a name with a prefix of your name/user-id. This is just to ensure that you are not saving the same with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. While creating the Python Function use the scoring URL and access token you got when you deployed the MMD model in one of the previous steps.","title":"Model Deployed in MMD further deployed as Python Function in WML"},{"location":"lab/3. Customer Churn Model Development and Deployment/3. Model Deployment and Release in IBM Cloud Public/1. Model Deployment in WML using API/","text":"Model Deployment using WML API Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab/3. Customer Churn Model Development and Deployment/3. Model Deployment and Release in IBM Cloud Public/1. Model Deployment in WML using API/#model-deployment-using-wml-api","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab/4. Weekly Batch Churn Scoring using the Deployed Model/1. Weekly Churn Scoring using Model Deployed directly in WML/","text":"Weekly Churn Scoring using Model Deployed in WML using WML API Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the direct Deployment you did to WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using Model Deployed in WML using WML API"},{"location":"lab/4. Weekly Batch Churn Scoring using the Deployed Model/1. Weekly Churn Scoring using Model Deployed directly in WML/#weekly-churn-scoring-using-model-deployed-in-wml-using-wml-api","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the direct Deployment you did to WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using Model Deployed in WML using WML API"},{"location":"lab/4. Weekly Batch Churn Scoring using the Deployed Model/2. Weekly Churn Scoring using MMD Model Deployed in WML/","text":"Weekly Churn Scoring using MMD Model Deployed in WML Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using MMD Model Deployed in WML"},{"location":"lab/4. Weekly Batch Churn Scoring using the Deployed Model/2. Weekly Churn Scoring using MMD Model Deployed in WML/#weekly-churn-scoring-using-mmd-model-deployed-in-wml","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using MMD Model Deployed in WML"},{"location":"lab/5. Monitoring Churn Model using Open Scale/1. Configuring Datamart and Deployment Infrastructure/","text":"Initial Configuration of Watson Open Scale In case you are using IBM Cloud Public you need to configure - Datamart - Here you need to provide Datamart's credential. Datamart can be either DB2Warehouse or Cloud Object Store or Postgres. Model Deployment environment - You need to select WML or any other deployment environment as used by you. If you are using IBM Cloud Pak for Data then this may be already configured for you. Otherwise, you would be automatically taken to the configuration screen where you need to configure as above. Please note that for IBM Cloud Pak for Data currently only DB2 supported as Datamart.","title":"Initial Configuration of Watson Open Scale"},{"location":"lab/5. Monitoring Churn Model using Open Scale/1. Configuring Datamart and Deployment Infrastructure/#initial-configuration-of-watson-open-scale","text":"In case you are using IBM Cloud Public you need to configure - Datamart - Here you need to provide Datamart's credential. Datamart can be either DB2Warehouse or Cloud Object Store or Postgres. Model Deployment environment - You need to select WML or any other deployment environment as used by you. If you are using IBM Cloud Pak for Data then this may be already configured for you. Otherwise, you would be automatically taken to the configuration screen where you need to configure as above. Please note that for IBM Cloud Pak for Data currently only DB2 supported as Datamart.","title":"Initial Configuration of Watson Open Scale"},{"location":"lab/5. Monitoring Churn Model using Open Scale/2. Adding Deployment and Configure Model/","text":"Adding Deployment and Configuring the Model If the initial configuration of Watson Open Scale is done, there would be Add Deployment button that can be used to add a specific deployment for monitoring purpose. This will bring up a screen showing list of available Deployments. This list must show the 2 Deployments you have created in previous steps. For Customer Churn use case add the Python Function Deployment (which internally access model deployed in MMD) by selecting the corresponding Deployment name shown in the available Deployment list. Next, for that Deployment you need to send few scoring requests. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Payload Scoring For Watson Open Scale\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While accessing the deployment use appropriate Deployment Id that corresponds to the Python Function Deployment . Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. After this step Watson Open Scale would allow you to configure the Model. For Configuring Model you need to - Provide Location of Training Data - In case of Cloud Pak For Data you have to specify any DB2 database for the same. For this lab specify the credential of internal DB2Wh of the Cloud Pak For Data (you should have it already as discussed in Getting Started step for IBM Cloud Pak for Data). In case of IBM Cloud Public you can use either DB2 database or Cloud Object Store. For this lab use DB2Wh in Cloud. (The credentials for the same are already discussed in Getting Started step for IBM Cloud Public). After you provide the location of Training Data, you need to select the schema and Table where the Training Data is stored. It should be same as the one you used in Model Development step. Identify Type of Inputs (the type of data used by model) - specify Numerical/Categorical Identify Algorithm Type - specify Binary Classification Identify the Label and Features of the Model from the attributes displayed to you. Also need to specify the attributes those are Categorical. Also you need Identify the attributes that you need to monitor as Prediction from the Model and the the attribute that identifies the Probability. Next you need to configure the Monitors - Configuring Monitor for Fairness - a) Identify Age and Gender are the attributes those you want to Monitor for Fairness. These two are sometimes automatically identified by Open Scale as attributes to be monitored for Fairness based on the training data. b) Use, 'F' (as False) as favorable outcome and 'T' (as True) as unfavorable outcome. c) For monitoring Gender, use 'F' (as Female) as Reference group and 'M' (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. d) For monitoring Age, use 40 to 60 as Reference group and 22 to 35 (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. Configuring Monitor for Accuracy - a) Use 95% as accuracy Threshold b) Use 50 as minimum sample and 200 as maximum sample c) Upload the 'customer_churn_wos_quality_feedback.csv' provided as feedback file.","title":"Adding Deployment and Configuring the Model"},{"location":"lab/5. Monitoring Churn Model using Open Scale/2. Adding Deployment and Configure Model/#adding-deployment-and-configuring-the-model","text":"If the initial configuration of Watson Open Scale is done, there would be Add Deployment button that can be used to add a specific deployment for monitoring purpose. This will bring up a screen showing list of available Deployments. This list must show the 2 Deployments you have created in previous steps. For Customer Churn use case add the Python Function Deployment (which internally access model deployed in MMD) by selecting the corresponding Deployment name shown in the available Deployment list. Next, for that Deployment you need to send few scoring requests. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Payload Scoring For Watson Open Scale\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While accessing the deployment use appropriate Deployment Id that corresponds to the Python Function Deployment . Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. After this step Watson Open Scale would allow you to configure the Model. For Configuring Model you need to - Provide Location of Training Data - In case of Cloud Pak For Data you have to specify any DB2 database for the same. For this lab specify the credential of internal DB2Wh of the Cloud Pak For Data (you should have it already as discussed in Getting Started step for IBM Cloud Pak for Data). In case of IBM Cloud Public you can use either DB2 database or Cloud Object Store. For this lab use DB2Wh in Cloud. (The credentials for the same are already discussed in Getting Started step for IBM Cloud Public). After you provide the location of Training Data, you need to select the schema and Table where the Training Data is stored. It should be same as the one you used in Model Development step. Identify Type of Inputs (the type of data used by model) - specify Numerical/Categorical Identify Algorithm Type - specify Binary Classification Identify the Label and Features of the Model from the attributes displayed to you. Also need to specify the attributes those are Categorical. Also you need Identify the attributes that you need to monitor as Prediction from the Model and the the attribute that identifies the Probability. Next you need to configure the Monitors - Configuring Monitor for Fairness - a) Identify Age and Gender are the attributes those you want to Monitor for Fairness. These two are sometimes automatically identified by Open Scale as attributes to be monitored for Fairness based on the training data. b) Use, 'F' (as False) as favorable outcome and 'T' (as True) as unfavorable outcome. c) For monitoring Gender, use 'F' (as Female) as Reference group and 'M' (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. d) For monitoring Age, use 40 to 60 as Reference group and 22 to 35 (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. Configuring Monitor for Accuracy - a) Use 95% as accuracy Threshold b) Use 50 as minimum sample and 200 as maximum sample c) Upload the 'customer_churn_wos_quality_feedback.csv' provided as feedback file.","title":"Adding Deployment and Configuring the Model"},{"location":"lab/5. Monitoring Churn Model using Open Scale/3. View Monitoring Results in Dashboard/","text":"Viewing Monitoring Results in Dashboard Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. Run the above Notebook to send enough scoring requests to WML. Now go to the Dashboard and select the tile of the Model you have already configured.","title":"Viewing Monitoring Results in Dashboard"},{"location":"lab/5. Monitoring Churn Model using Open Scale/3. View Monitoring Results in Dashboard/#viewing-monitoring-results-in-dashboard","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. Run the above Notebook to send enough scoring requests to WML. Now go to the Dashboard and select the tile of the Model you have already configured.","title":"Viewing Monitoring Results in Dashboard"},{"location":"lab/5. Monitoring Churn Model using Open Scale/4. Access Monitoring Results by using APIs/","text":"Model Deployment in MMD of Cloud Pak for Data Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Deployment in MMD of Cloud Pak for Data"},{"location":"lab/5. Monitoring Churn Model using Open Scale/4. Access Monitoring Results by using APIs/#model-deployment-in-mmd-of-cloud-pak-for-data","text":"Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Deployment in MMD of Cloud Pak for Data"},{"location":"lab/6. Watson Assistant to develop Chatbot/1. Data Ingestion for Watson Assistant/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"1. Data Ingestion for Watson Assistant"},{"location":"lab/6. Watson Assistant to develop Chatbot/2. Calling Watson Natural Language Understanding for Tone Analysis/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"2. Calling Watson Natural Language Understanding for Tone Analysis"},{"location":"lab/6. Watson Assistant to develop Chatbot/3. On Demand Churn Scoring and Updating/","text":"Model Development has following sub steps. Data Ingestion Data Preparation Model Development Model Deployment Releasing the Model for Scoring","title":"3. On Demand Churn Scoring and Updating"},{"location":"lab-IBMCloudPakForData/readme/","text":"Customer Churn Lab for Cloud Pak for Data Step 1: Getting Started 1. Dataset and sample code 2. Dataset and Initial Setups for IBM Cloud Pak for Data Step 2: Data Ingestion, Organization and Governance for Customer Churn 1. Data Discovery 2. Data Governance - Creating Business Glossary, Policies and Rules 3. Data Ingestion from Flat Files to a Project 4. Data Ingestion from Flat Files to a Project Step 3: Customer Churn Model Development and Deployment 1. Model Development using PySpark in Jupyter Notebook 2. Model Development using Auto AI 3. Model Development using SPSS 4. Model Deployment in MMD 5. Model Deployment in WML using API 6. Python Function accessing Model in MMD Deployed in WML using API Step 4: Weekly Churn Scoring using the deployed Model 1. Churn Scoring using Model Deployed in WML 2. Churn Scoring using MMD Model function Deployed in WML Step 5: Monitoring Churn Model using Open Scale 1. Configuring Datamart and Deployment Infrastructure 2. Adding Deployment and Configuring Model 3. View Monitoring Results in Dashboard 4. Access Monitoring Results using API","title":"IBM Cloud Pak for Data"},{"location":"lab-IBMCloudPakForData/readme/#customer-churn-lab-for-cloud-pak-for-data","text":"","title":"Customer Churn Lab for Cloud Pak for Data"},{"location":"lab-IBMCloudPakForData/readme/#step-1-getting-started","text":"1. Dataset and sample code 2. Dataset and Initial Setups for IBM Cloud Pak for Data","title":"Step 1: Getting Started"},{"location":"lab-IBMCloudPakForData/readme/#step-2-data-ingestion-organization-and-governance-for-customer-churn","text":"1. Data Discovery 2. Data Governance - Creating Business Glossary, Policies and Rules 3. Data Ingestion from Flat Files to a Project 4. Data Ingestion from Flat Files to a Project","title":"Step 2: Data Ingestion, Organization and Governance for Customer Churn"},{"location":"lab-IBMCloudPakForData/readme/#step-3-customer-churn-model-development-and-deployment","text":"1. Model Development using PySpark in Jupyter Notebook 2. Model Development using Auto AI 3. Model Development using SPSS 4. Model Deployment in MMD 5. Model Deployment in WML using API 6. Python Function accessing Model in MMD Deployed in WML using API","title":"Step 3: Customer Churn Model Development and Deployment"},{"location":"lab-IBMCloudPakForData/readme/#step-4-weekly-churn-scoring-using-the-deployed-model","text":"1. Churn Scoring using Model Deployed in WML 2. Churn Scoring using MMD Model function Deployed in WML","title":"Step 4: Weekly Churn Scoring using the deployed Model"},{"location":"lab-IBMCloudPakForData/readme/#step-5-monitoring-churn-model-using-open-scale","text":"1. Configuring Datamart and Deployment Infrastructure 2. Adding Deployment and Configuring Model 3. View Monitoring Results in Dashboard 4. Access Monitoring Results using API","title":"Step 5: Monitoring Churn Model using Open Scale"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Deployment in WML using API/","text":"Model Deployment using WML API Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Deployment in WML using API/#model-deployment-using-wml-api","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy Model Using WML API\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Deploying the Model using deployment name with a prefix of your name/user-id. This is just to ensure that you are not deploying a model with a deployment name same as others used. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Deployment using WML API"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Development in MMD/","text":"Model Deployment in MMD of Cloud Pak for Data Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Development in MMD"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Development in MMD/#model-deployment-in-mmd-of-cloud-pak-for-data","text":"Prepare the model artifacts for deployment in production setup so that we can score new records. The data scientists may tag a model once satisfied with the result as \u201cpublished\u201d for production consumption. In ICP for Data platform this step is called creating \u201cProject release\u201d. Navigate to the project homepage by clicking the Project Name. Click on \u201cGit actions\u201d and click \u201cCommit\u201d from top right menu (similar to that shown in the diagram below). This will (commit) all of the assets to the project repository. The \u201ccommit\u201d screen will show list of the assets that are created in this project (similar to that shown in the diagram below). Enter suitable comment for e.g. \u201cThis is initial project release by user7\u201d. Click \u2018Commit \u2018. You will get a message window confirming the changes were commited and now available to \u201cpush\u201d for deployment. Click on \u201cpush\u201d by clicking on \u201cGit actions\u201d and select \u201cpush\u201d (similar to that shown in the diagram below). We need to create a \u201ctag\u201d for our release, so that a version of this model can be identified for deployment. For e.g. enter tag as \u201ctelcochurn \u201d. Now a Deployer can start working on Deploying the model. For that he/she has to first create a release for this project. A project release represents a project tag that can be launched as a production environment. The deployer can monitor releases and deployments from the Project releases page. To allow him/her do the same, first you have to make the Deployer a collaborator in your project. You can add an existing user of this platform as collaborator to this project by going the Collaborator tab of the project and adding a Collaborator (similar to that shown in the diagram below). Once you click the Add Collaborator you shall see the name of all other users. Add the Data Engineer UserId. Give that person Admin privilege for the project in the Permission column. Click the \u2018Add\u2019 button. The deployer has to login and go to the Project List by clicking the Projects in the left navigation menu. The project you created as other user would appear there. By clicking the project name, the deployer would go to the home page of the project. Next he has to Click the \u2018Pull\u2019 from the Repository icon drop down on the right top of the project home page. He/She may get the message saying you are up to date. However, for this Lab you can use your user id to play the role of the Deployer and execute the subsequent steps. Navigate to the home menu and select \u2018Model Deployments\u2019 under \u2018Administer\u2019 from the left navigation menu. It will take you to the Project Release page. To add a new project release Click \u201cAdd Project Release \u201c from right hand side option. Add in the details of your Project Release under the \u2018From Watson Studio\u2019 tab. These details should be unique. For e.g. Name : user7telcochurnprjrel Route : user7telcochurnprjroute Source Project : TelcoChurnUser7Prj (This is the project you are working so far on) Tag : telcochurnuser7v1 (this is the tage you gave entered while pushing the project) The name can contain hyphens but not special characters such as a period (.). The route is the unique ID for the project release, and is used within the deployments' REST paths and URLs. This is a unique part of the url that will be assigned to all of the assets that are created related with this project. Name of the \u201croute\u201d must be lowercase. Please remember the route you provided in this step. Click \u2018Create\u2019 . This creates an offline release. Now, Click on \u201cProject releases\u201d from top left. This will show currently available releases. Note that your view shows 0 under Deployments. We will be deploying a model asset within this project release. To view details of the deployed project, from Project release page, click on deployment name that was just created for e.g. \u201cuser7prjrel\u201d. This will bring up Dashboard for the deployment with several detailed tabs for more information about the Project release. Click on Asset tab to see all deployed assets related to this Project release if the system does not take you there automatically. Next we will create a \u201cRealtime online deployment\u201d of the model. The Cloud Pak for Data platform will package the required model artifacts and package it as a docker container along with any runtime execution environment. This container is isolated image from other \u201cdevelopment\u201d services that are running. An administrator can provision multiple replicas of this service in order to full-fill concurrent access to model scoring in production. In the Assets tab, you can see all of the analytics assets. There are notebooks, models, and scripts that we created previously. Selct the model with the name that you created in model creation step, (say something like \u2018Telco Churn Model \u2019) on the left and Click \u201cweb service\u201d. This will bring up \u201cDeploy\u201d window expecting input as shown below. Fill in the deployment details. For example : Name : user7telcochurndepv1 (it should contain only lower case letter and number) Model version : 1 Web service environment : Python 3.6 \u2013 Script as Service. Please note that programing language and its version of the environment you select here should be similar to the one you have used for creating the model. For example, if you have used Python 3.6 Spark 2.3.2 as environment while developing the model here the environment you select should be Python 3.6 \u2013 Script as Service Click \u201cCreate\u201d. This will bring deployment Overview screen with API tab as shown below. Note your Web service ENDPOINT URL and access token. At this time, the online deployment is created. You could also find the REST API and deployment token under \u2018API\u2019 tab. Simply click, the token is copied to your clipboard. However, note that the deployment is not active as yet. We will need to \u201cLaunch\u201d the deployment in order instantiate required execution environments. Click on Project releases tab from top left side tab. Click on 3 dots right of project release name. You will see screen options as similar to below. Click \u201cLaunch\u201d . This will prompt showing all deployments will be taken online for end user access. Click \u201cLaunch\u201d again in the current screen. This will show confirmation message that deployment is successfully brought online. Actual instantiantion of environments and setup could take 1-2 minutes before APIs can be accessible. Under project releases , \u201cDeployments\u201d will show count as 1. Click on Project release name. This will bring back Dashboard. Click on Deploments tab, now this will show 1 model with \u201cAvaliablity\u201d as \u201cEnabled\u201d . (The enabled status may not change for 1-2 minutes after \u201cLaunch\u201d step. Allow some time, it it is still in \u201cdisable\u201d state. May need to click to different tab to update status.). Click on deployment name ( for e.g. user7depv1 ). This will bring up \u201cOverview\u201d and API tab for this deployment. To test the endpoint is functioning, we can run a sample record to score. Click on API tab and Click \u201cSubmit\u201d under \u201cBody\u201d window. This will show scoring of 1 record using real-time deployment of the model. The Response is shown as JSON record. You are here mimicing the invokation of the Rest API for scoring the model as you would do to for online real time Scoring. Note: Generate Code option on top right shows how to invoke REST API call using cURL command. Your Customer Churn model asset was developed, trained, and deployed for production scoring usage. You have also invoked the model\u2019s scoring API real time as you would call it for online real time Scoring.","title":"Model Deployment in MMD of Cloud Pak for Data"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Development using PySpark in Jupyter Notebook/","text":"Model Development using PySpark in Jupyter Notebook Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Create and Save Model\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - You can read the data either from a Database/DataWarehouse or Virtual Datasource or Flat File depending on your previous step for Data Ingestion and Data Organization. Accordingly you run appropriate cells in the notebook While Saving the Model using Model name with a prefix of your name/user-id. This is just to ensure that you are not saving a model with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Development using PySpark in Jupyter Notebook"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Model Development using PySpark in Jupyter Notebook/#model-development-using-pyspark-in-jupyter-notebook","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Create and Save Model\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - You can read the data either from a Database/DataWarehouse or Virtual Datasource or Flat File depending on your previous step for Data Ingestion and Data Organization. Accordingly you run appropriate cells in the notebook While Saving the Model using Model name with a prefix of your name/user-id. This is just to ensure that you are not saving a model with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Model Development using PySpark in Jupyter Notebook"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Python Function accessing Model in MMD Deployed in WML using API/","text":"Model Deployed in MMD further deployed as Python Function in WML This is to showcase the situation where Model deployed in any arbitrary environment (say Amazon Sage Maker, Azure Ml, Custom Environment, IBM WML in Cloud, etc.) needs to be further deployed in WML in Cloud Pak for Data. This is needed for Watson Open Scale being able to monitor models deployed in any arbitrary deployment environment for AI models. Here we are using MMD as example of that arbitrary Model deployment environment. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy MMD Model as Python Function in WML\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While saving and deploying the Python Function use a name with a prefix of your name/user-id. This is just to ensure that you are not saving the same with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. While creating the Python Function use the scoring URL and access token you got when you deployed the MMD model in one of the previous steps.","title":"Model Deployed in MMD further deployed as Python Function in WML"},{"location":"lab-IBMCloudPakForData/Customer Churn Model Development and Deployment/Python Function accessing Model in MMD Deployed in WML using API/#model-deployed-in-mmd-further-deployed-as-python-function-in-wml","text":"This is to showcase the situation where Model deployed in any arbitrary environment (say Amazon Sage Maker, Azure Ml, Custom Environment, IBM WML in Cloud, etc.) needs to be further deployed in WML in Cloud Pak for Data. This is needed for Watson Open Scale being able to monitor models deployed in any arbitrary deployment environment for AI models. Here we are using MMD as example of that arbitrary Model deployment environment. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Deploy MMD Model as Python Function in WML\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While saving and deploying the Python Function use a name with a prefix of your name/user-id. This is just to ensure that you are not saving the same with a name same as others used. Whereever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. While creating the Python Function use the scoring URL and access token you got when you deployed the MMD model in one of the previous steps.","title":"Model Deployed in MMD further deployed as Python Function in WML"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Discovery/","text":"Data Discovery - Check for Relevant Data Assets","title":"Data Discovery - Check for Relevant Data Assets"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Discovery/#data-discovery-check-for-relevant-data-assets","text":"","title":"Data Discovery - Check for Relevant Data Assets"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Governance - Creating Business Glossary, Policies and Rules/","text":"Data Governance - Creating Business Glossary, Policies and Rules.md","title":"Data Governance   Creating Business Glossary, Policies and Rules"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Governance - Creating Business Glossary, Policies and Rules/#data-governance-creating-business-glossary-policies-and-rulesmd","text":"","title":"Data Governance - Creating Business Glossary, Policies and Rules.md"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Ingestion from Flat Files to a Project/","text":"Data Ingestion from Flat Files to a Project If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all data files available in Datasets folder at the root of this repository. b) Next, click Datasets under Assets. In the resulting page click Add Dataset. c) In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Ingestion from Flat Files to a Project/#data-ingestion-from-flat-files-to-a-project","text":"If you are using Sample Project necessary Flat Files are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all data files available in Datasets folder at the root of this repository. b) Next, click Datasets under Assets. In the resulting page click Add Dataset. c) In that page under Local File tab click \u2018Select From Your Local File System\u2019 button. Upload the file customer_usage_history.csv that you have downloaded from the Dataset folder. Repeat the above step to upload the remaining files.","title":"Data Ingestion from Flat Files to a Project"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Organization using PySpark in Notebook/","text":"Data Organization using PySpark in Jupyter Notebook If you are using Sample Project necessary Notebooks are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all Notebook files available in Notebooks folder at the root of this repository. b) Next, click Notebooks under Assets. In the resulting page click Add Notebook. c) Go to From File tab. Upload the notebook 'Prepare and Save Data For Building a ML Model'. Next, open the Notebook using Spark Environment. Use the highest version of Spark Environment to open the Notebook by clicking three vertical dots at the right of the notebook name). Run all cells of the Notebook following the Instructions in the Notebook.","title":"Data Organization using PySpark in Jupyter Notebook"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/Data Organization using PySpark in Notebook/#data-organization-using-pyspark-in-jupyter-notebook","text":"If you are using Sample Project necessary Notebooks are already ingested in your Project. Otherwise, a) First ensure that you have downloaded all Notebook files available in Notebooks folder at the root of this repository. b) Next, click Notebooks under Assets. In the resulting page click Add Notebook. c) Go to From File tab. Upload the notebook 'Prepare and Save Data For Building a ML Model'. Next, open the Notebook using Spark Environment. Use the highest version of Spark Environment to open the Notebook by clicking three vertical dots at the right of the notebook name). Run all cells of the Notebook following the Instructions in the Notebook.","title":"Data Organization using PySpark in Jupyter Notebook"},{"location":"lab-IBMCloudPakForData/Data Ingestion, Organization and Governance for Customer Churn/images/readme/","text":"","title":"Readme"},{"location":"lab-IBMCloudPakForData/Getting Started/Datasets and Sample Code/","text":"Datasets and Sample Code to be used to develop this Use case All necessary datasets to develop this Use case are available in the 'datasets' folder at the root of this repository - https://github.com/ibm-cloud-architecture/refarch-ai-data-customer-churn. You have to download them before hand for development of this Use case. Sample Codes are available in Notebooks in the 'Notebook' folder at the root of this same repository. You have to download them before hand for development of this Use case. There are 2 subfolders under Notebook folder. One has Notebooks for IBM Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. You have to download from the folder for IBM Cloud Pak for Data for this lab. There are also Sample Projects available which contains Datasets, Sample Notebooks and other artifacts. You can upload them as a new project (with From File option) in IBM Cloud Pak for Data instance. These sample projects are available in 'Sample Projects' folder at the root of the same repository. There are 2 subfolders under Sample Projects folder. One has project zip file for IBM Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. You have to download from the folder for IBM Cloud Pak for Data for this lab.","title":"Datasets and Sample Code to be used to develop this Use case"},{"location":"lab-IBMCloudPakForData/Getting Started/Datasets and Sample Code/#datasets-and-sample-code-to-be-used-to-develop-this-use-case","text":"All necessary datasets to develop this Use case are available in the 'datasets' folder at the root of this repository - https://github.com/ibm-cloud-architecture/refarch-ai-data-customer-churn. You have to download them before hand for development of this Use case. Sample Codes are available in Notebooks in the 'Notebook' folder at the root of this same repository. You have to download them before hand for development of this Use case. There are 2 subfolders under Notebook folder. One has Notebooks for IBM Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. You have to download from the folder for IBM Cloud Pak for Data for this lab. There are also Sample Projects available which contains Datasets, Sample Notebooks and other artifacts. You can upload them as a new project (with From File option) in IBM Cloud Pak for Data instance. These sample projects are available in 'Sample Projects' folder at the root of the same repository. There are 2 subfolders under Sample Projects folder. One has project zip file for IBM Cloud Pak for Data, the other has same for Watson Studio in IBM Cloud Public. You have to download from the folder for IBM Cloud Pak for Data for this lab.","title":"Datasets and Sample Code to be used to develop this Use case"},{"location":"lab-IBMCloudPakForData/Getting Started/Initial Setups/","text":"Initial Setup for Cloud Pak for Data Cloud Pak for Data should be either installed for developing this use case or you have to install the same using the instructions available . Once Cloud Pak for Data is installed, various users need to be created. These users should have been already created with necessary access rights. You should have following information available with you for developing this use case - a. Cloud Pak for Data - URL b. Watson Open Scale add on for Cloud Oak for Data - URL c. JDBC URL, Login id and Password for DB2 Wearhouse internal to Cloud Oak for Data d. Credential of a User Profile (user id/password) that has access right of Administrator, Data Engineer, Business Analyst, Data Scientist, Data Steward and App Developer Use the above information to login to Cloud Oak for Data UI and Watson Open Scale UI to ensure that you can get started. On successful login you will see a welcome page as below Next, you should create a Global Connection for DB2 Wearhouse internal to Cloud Oak for Data using following steps - Click on Connections in the left Navigation menu. Connections allow you to create data connections and browse the data connections that you can use to add data to the enterprise governance catalog, analyze data, virtualize data, and more. Click \u2018Add Connection\u2019. The \u2018Add Connection\u2019 will open a screen as below. Provide all the details. Then you can test the connection using \u201cTest Connection\u201d button at the right bottom of the screen. If the \u201cTest Connection\u201d step is successful \u2018Save\u2019 the same. One can also upload custom driver if the data source is not available in the drop-down list of out-of-the box sources. Next, create a Project to get started. A project is a collection of assets that you use to achieve a particular data analysis goal. Your project assets can include: Notebooks RStudio files Models Data assets (local files, data sources, and remote data sets) Scripts Login with your UserId. Go to Projects from the left Navigation Menu. Then click on Add Project. This would show you the Create Project screen below. If you have not downloaded already, please download the Sample Project file from the Cloud Oak for Data folder under Sample Projects in the root of this repository. After download rename downloaded file to TelcoChurnPrj .zip. To import project select \u2018From file\u2019. drag and drop or browse for the Zip file you downloaded earlier (TelcoChurnPrj .zip). Click \u2018Create\u2019, then the project is created. It will take you to the \u201cAssets\u201d page of this project. Alternatively, you can also create a blank project by using the first left tab and providing the same name as above. Later on you need to add datasets and Notebooks to this project.","title":"Initial Setup for Cloud Pak for Data"},{"location":"lab-IBMCloudPakForData/Getting Started/Initial Setups/#initial-setup-for-cloud-pak-for-data","text":"Cloud Pak for Data should be either installed for developing this use case or you have to install the same using the instructions available . Once Cloud Pak for Data is installed, various users need to be created. These users should have been already created with necessary access rights. You should have following information available with you for developing this use case - a. Cloud Pak for Data - URL b. Watson Open Scale add on for Cloud Oak for Data - URL c. JDBC URL, Login id and Password for DB2 Wearhouse internal to Cloud Oak for Data d. Credential of a User Profile (user id/password) that has access right of Administrator, Data Engineer, Business Analyst, Data Scientist, Data Steward and App Developer Use the above information to login to Cloud Oak for Data UI and Watson Open Scale UI to ensure that you can get started. On successful login you will see a welcome page as below Next, you should create a Global Connection for DB2 Wearhouse internal to Cloud Oak for Data using following steps - Click on Connections in the left Navigation menu. Connections allow you to create data connections and browse the data connections that you can use to add data to the enterprise governance catalog, analyze data, virtualize data, and more. Click \u2018Add Connection\u2019. The \u2018Add Connection\u2019 will open a screen as below. Provide all the details. Then you can test the connection using \u201cTest Connection\u201d button at the right bottom of the screen. If the \u201cTest Connection\u201d step is successful \u2018Save\u2019 the same. One can also upload custom driver if the data source is not available in the drop-down list of out-of-the box sources. Next, create a Project to get started. A project is a collection of assets that you use to achieve a particular data analysis goal. Your project assets can include: Notebooks RStudio files Models Data assets (local files, data sources, and remote data sets) Scripts Login with your UserId. Go to Projects from the left Navigation Menu. Then click on Add Project. This would show you the Create Project screen below. If you have not downloaded already, please download the Sample Project file from the Cloud Oak for Data folder under Sample Projects in the root of this repository. After download rename downloaded file to TelcoChurnPrj .zip. To import project select \u2018From file\u2019. drag and drop or browse for the Zip file you downloaded earlier (TelcoChurnPrj .zip). Click \u2018Create\u2019, then the project is created. It will take you to the \u201cAssets\u201d page of this project. Alternatively, you can also create a blank project by using the first left tab and providing the same name as above. Later on you need to add datasets and Notebooks to this project.","title":"Initial Setup for Cloud Pak for Data"},{"location":"lab-IBMCloudPakForData/Getting Started/images/readme/","text":"","title":"Readme"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Access Monitoring Results using API/","text":"Access Monitoring Results using API.md","title":"Access Monitoring Results using API.md"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Access Monitoring Results using API/#access-monitoring-results-using-apimd","text":"","title":"Access Monitoring Results using API.md"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Adding Deployment and Configuring Model/","text":"Adding Deployment and Configuring the Model If the initial configuration of Watson Open Scale is done, there would be Add Deployment button that can be used to add a specific deployment for monitoring purpose. This will bring up a screen showing list of available Deployments. This list must show the 2 Deployments you have created in previous steps. For Customer Churn use case add the Python Function Deployment (which internally access model deployed in MMD) by selecting the corresponding Deployment name shown in the available Deployment list. Next, for that Deployment you need to send few scoring requests. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Payload Scoring For Watson Open Scale\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While accessing the deployment use appropriate Deployment Id that corresponds to the Python Function Deployment . Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. After this step Watson Open Scale would allow you to configure the Model with the message that Logging Activated Successfully. For Configuring Model you need to first click on Model Deatils and select Manually Configure Monitors. Provide Location of Training Data - In case of Cloud Pak For Data you have to specify any DB2 database for the same. For this lab specify the credential of internal DB2Wh of the Cloud Pak For Data (you should have it already as discussed in Getting Started step for IBM Cloud Pak for Data). After you provide the location of Training Data, you need to select the schema and Table where the Training Data is stored. It should be same as the one you used in Model Development step. Identify Type of Inputs (the type of data used by model) - specify Numerical/Categorical Identify Algorithm Type - specify Binary Classification Identify the Label and Features of the Model from the attributes displayed to you. Also need to specify the attributes those are Categorical. Also you need Identify the attributes that you need to monitor as Prediction from the Model and the the attribute that identifies the Probability. Next you need to configure the Monitors - Configuring Monitor for Fairness - a) Identify Age and Gender are the attributes those you want to Monitor for Fairness. These two are sometimes automatically identified by Open Scale as attributes to be monitored for Fairness based on the training data. b) Use, 'F' (as False) as favorable outcome and 'T' (as True) as unfavorable outcome. c) For monitoring Gender, use 'F' (as Female) as Reference group and 'M' (as Male) as Monitored group. Use 97% as threshold for Bias. And use 50 as the minimum sample of records to be used. d) For monitoring Age, use 40 to 60 as Reference group and 22 to 35 (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. Configuring Monitor for Quality - a) Use 95% as accuracy Threshold b) Use 50 as minimum sample and 200 as maximum sample c) Upload the 'customer_churn_quality_feedback.csv' provided as feedback file.","title":"Adding Deployment and Configuring the Model"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Adding Deployment and Configuring Model/#adding-deployment-and-configuring-the-model","text":"If the initial configuration of Watson Open Scale is done, there would be Add Deployment button that can be used to add a specific deployment for monitoring purpose. This will bring up a screen showing list of available Deployments. This list must show the 2 Deployments you have created in previous steps. For Customer Churn use case add the Python Function Deployment (which internally access model deployed in MMD) by selecting the corresponding Deployment name shown in the available Deployment list. Next, for that Deployment you need to send few scoring requests. Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Payload Scoring For Watson Open Scale\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While accessing the deployment use appropriate Deployment Id that corresponds to the Python Function Deployment . Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. After this step Watson Open Scale would allow you to configure the Model with the message that Logging Activated Successfully. For Configuring Model you need to first click on Model Deatils and select Manually Configure Monitors. Provide Location of Training Data - In case of Cloud Pak For Data you have to specify any DB2 database for the same. For this lab specify the credential of internal DB2Wh of the Cloud Pak For Data (you should have it already as discussed in Getting Started step for IBM Cloud Pak for Data). After you provide the location of Training Data, you need to select the schema and Table where the Training Data is stored. It should be same as the one you used in Model Development step. Identify Type of Inputs (the type of data used by model) - specify Numerical/Categorical Identify Algorithm Type - specify Binary Classification Identify the Label and Features of the Model from the attributes displayed to you. Also need to specify the attributes those are Categorical. Also you need Identify the attributes that you need to monitor as Prediction from the Model and the the attribute that identifies the Probability. Next you need to configure the Monitors - Configuring Monitor for Fairness - a) Identify Age and Gender are the attributes those you want to Monitor for Fairness. These two are sometimes automatically identified by Open Scale as attributes to be monitored for Fairness based on the training data. b) Use, 'F' (as False) as favorable outcome and 'T' (as True) as unfavorable outcome. c) For monitoring Gender, use 'F' (as Female) as Reference group and 'M' (as Male) as Monitored group. Use 97% as threshold for Bias. And use 50 as the minimum sample of records to be used. d) For monitoring Age, use 40 to 60 as Reference group and 22 to 35 (as Male) as Monitored group. Use 97% as threshold for Bias. And use 40 as the sample of records to be used. Configuring Monitor for Quality - a) Use 95% as accuracy Threshold b) Use 50 as minimum sample and 200 as maximum sample c) Upload the 'customer_churn_quality_feedback.csv' provided as feedback file.","title":"Adding Deployment and Configuring the Model"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Configuring Datamart and Deployment Infrastructure/","text":"Initial Configuration of Watson Open Scale If you are using IBM Cloud Pak for Data then this may be already configured for you. Otherwise, you would be automatically taken to the configuration screen where you need to configure as below. Please note that for IBM Cloud Pak for Data currently only DB2 supported as Datamart. Datamart - Here you need to provide Datamart's credential. Datamart can be either DB2Warehouse. Model Deployment environment - You need to select WML or any other deployment environment as used by you.","title":"Initial Configuration of Watson Open Scale"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/Configuring Datamart and Deployment Infrastructure/#initial-configuration-of-watson-open-scale","text":"If you are using IBM Cloud Pak for Data then this may be already configured for you. Otherwise, you would be automatically taken to the configuration screen where you need to configure as below. Please note that for IBM Cloud Pak for Data currently only DB2 supported as Datamart. Datamart - Here you need to provide Datamart's credential. Datamart can be either DB2Warehouse. Model Deployment environment - You need to select WML or any other deployment environment as used by you.","title":"Initial Configuration of Watson Open Scale"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/View Monitoring Results in Dashboard/","text":"Viewing Monitoring Results in Dashboard Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. Run the above Notebook to send enough scoring requests to WML. Now go to the Dashboard and select the tile of the Model you have already configured. Click the same and it will take you to the detailed view of the monitored result. Click on, one of the Attribute you are monitoring for Fairness. It will take you to the Fairness Screen. There you can click on a particul;ar point on the line. That will take you to the deatils of the Fairness. From this screen you can select View Transactions button. That will take you to the Transaction Details screen. There you can click on Explain link for any transaction. That would generate Explaination for that transaction. It may take few minutes for WoS to generate the Explanation Now go back to the Dashboard and clickn on Tile of your deployment again. Click on Quality/Area Uner RoC. It will show you the Quality with respect to Threshold you have set in configuration step before. If you click on the line of Quality (green line) you can see the details of the Quality.","title":"Viewing Monitoring Results in Dashboard"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/View Monitoring Results in Dashboard/#viewing-monitoring-results-in-dashboard","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you. Run the above Notebook to send enough scoring requests to WML. Now go to the Dashboard and select the tile of the Model you have already configured. Click the same and it will take you to the detailed view of the monitored result. Click on, one of the Attribute you are monitoring for Fairness. It will take you to the Fairness Screen. There you can click on a particul;ar point on the line. That will take you to the deatils of the Fairness. From this screen you can select View Transactions button. That will take you to the Transaction Details screen. There you can click on Explain link for any transaction. That would generate Explaination for that transaction. It may take few minutes for WoS to generate the Explanation Now go back to the Dashboard and clickn on Tile of your deployment again. Click on Quality/Area Uner RoC. It will show you the Quality with respect to Threshold you have set in configuration step before. If you click on the line of Quality (green line) you can see the details of the Quality.","title":"Viewing Monitoring Results in Dashboard"},{"location":"lab-IBMCloudPakForData/Monitoring Churn Model using Open Scale/images/readme/","text":"","title":"Readme"},{"location":"lab-IBMCloudPakForData/Weekly Churn Scoring using the deployed Model/Churn Scoring using MMD Model function Deployed in WML/","text":"Weekly Churn Scoring using MMD Model Deployed in WML Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using MMD Model Deployed in WML"},{"location":"lab-IBMCloudPakForData/Weekly Churn Scoring using the deployed Model/Churn Scoring using MMD Model function Deployed in WML/#weekly-churn-scoring-using-mmd-model-deployed-in-wml","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the Python Function (accessing the MMD Model) that you deployed in WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using MMD Model Deployed in WML"},{"location":"lab-IBMCloudPakForData/Weekly Churn Scoring using the deployed Model/Churn Scoring using Model Deployed in WML/","text":"Weekly Churn Scoring using Model Deployed in WML using WML API Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the direct Deployment you did to WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using Model Deployed in WML using WML API"},{"location":"lab-IBMCloudPakForData/Weekly Churn Scoring using the deployed Model/Churn Scoring using Model Deployed in WML/#weekly-churn-scoring-using-model-deployed-in-wml-using-wml-api","text":"Navigate to the \u2018Notebooks\u2019 tab of your project and open \u2018Customer Churn Batch Scoring\u2019 notebook using Jupyter with latest/highest version of Spark environment. You can select the environment by clicking the 3 vertical dots at the right of the name of the Notebook. Follow the notebook instructions and execute all cells as directed. Please note following - While Accessing the Model use Deployment ID that corresponds to the direct Deployment you did to WML in one of the previous steps. Wherever, user credentials are needed please use your user if and password Wherever, urls are needed use the Cloud Pak for Data URL (ip/host name and port) provided to you.","title":"Weekly Churn Scoring using Model Deployed in WML using WML API"}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook we shall create a Machine Learning Model using Spark and Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: watson_machine_learning_client==1.0.375 in /user-home/1001/.local/lib/python3.6/site-packages (1.0.375)\n",
      "Requirement already satisfied: tabulate in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (0.8.3)\n",
      "Requirement already satisfied: pandas in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (0.24.2)\n",
      "Requirement already satisfied: certifi in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (2019.3.9)\n",
      "Requirement already satisfied: lomond in /user-home/_global_/python-3 (from watson_machine_learning_client==1.0.375) (0.3.3)\n",
      "Requirement already satisfied: requests in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (2.22.0)\n",
      "Requirement already satisfied: ibm-cos-sdk in /user-home/_global_/python-3 (from watson_machine_learning_client==1.0.375) (2.5.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda3/lib/python3.6/site-packages (from watson_machine_learning_client==1.0.375) (4.32.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda3/lib/python3.6/site-packages (from pandas->watson_machine_learning_client==1.0.375) (1.14.6)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda3/lib/python3.6/site-packages (from pandas->watson_machine_learning_client==1.0.375) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda3/lib/python3.6/site-packages (from pandas->watson_machine_learning_client==1.0.375) (2019.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda3/lib/python3.6/site-packages (from lomond->watson_machine_learning_client==1.0.375) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda3/lib/python3.6/site-packages (from requests->watson_machine_learning_client==1.0.375) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda3/lib/python3.6/site-packages (from requests->watson_machine_learning_client==1.0.375) (2.8)\n",
      "Requirement already satisfied: ibm-cos-sdk-core>=2.0.0 in /user-home/_global_/python-3 (from ibm-cos-sdk->watson_machine_learning_client==1.0.375) (2.5.2)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer>=2.0.0 in /user-home/_global_/python-3 (from ibm-cos-sdk->watson_machine_learning_client==1.0.375) (2.5.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda3/lib/python3.6/site-packages (from ibm-cos-sdk->watson_machine_learning_client==1.0.375) (0.9.4)\n",
      "Requirement already satisfied: docutils>=0.10 in /opt/conda3/lib/python3.6/site-packages (from ibm-cos-sdk-core>=2.0.0->ibm-cos-sdk->watson_machine_learning_client==1.0.375) (0.14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --user watson_machine_learning_client==1.0.375\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The key reason we are using PySpark here is to show how to create a Machine Learning Model using large volume of data that may not fit within the memory of single Python process. So we need distributed computing infrastructure for those cases. We are using Spark here as a distributed computing Infrastructure. PySpark is a library that helps using Spark's capability using Python as programming language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we import PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"WOSM\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we read in a dataset that we will use to develop a Machine Learning model. Once we read in the data, it can potentially reside on multiple machines/VMs in a distributed fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can read the data here in various ways. We are showing here two mechanisms - reading data from a CSV file and reading data from a Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are reading the data from CSV file, read the data you have stored in previous step using the same file name in the next cell. Otherwise skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "| ID|LONGDISTANCE|INTERNATIONAL|LOCAL|DROPPED|PAYMETHOD|LOCALBILLTYPE|LONGDISTANCEBILLTYPE|USAGE|RATEPLAN|CHURN|GENDER|STATUS|CHILDREN|ESTINCOME|CAROWNER|      AGE|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "|  1|          23|            0|  206|      0|       CC|       Budget|      Intnl_discount|  229|       3|    T|     F|     S|       1|  38000.0|       N|24.393333|\n",
      "|  6|          29|            0|   45|      0|       CH|    FreeLocal|            Standard|   75|       2|    F|     M|     M|       2|  29616.0|       N|49.426667|\n",
      "|  8|          24|            0|   22|      0|       CC|    FreeLocal|            Standard|   47|       3|    F|     M|     M|       0|  19732.8|       N|50.673333|\n",
      "| 11|          26|            0|   32|      1|       CC|       Budget|            Standard|   59|       1|    F|     M|     S|       2|    96.33|       N|56.473333|\n",
      "| 17|          12|            0|   46|      4|       CC|    FreeLocal|            Standard|   58|       1|    F|     M|     M|       2|  53010.8|       N|    18.84|\n",
      "+---+------------+-------------+-----+-------+---------+-------------+--------------------+-----+--------+-----+------+------+--------+---------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmergedDf = spark.read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/enhanced_customers_history.csv', header='true')\n",
    "cmergedDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are reading the data from the Database read the data from the same using the instruction provided in the Hands On Lab document. Otherwise skip the next 2 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dsx_core_utils, requests, os, io\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# # Add asset from remote connection\n",
    "# df83 = None\n",
    "# dataSet = dsx_core_utils.get_remote_data_set_info('USER1009.CUSTOMER_MERGED_HISTORY_VIEW')\n",
    "# dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "# sparkSession = SparkSession(sc).builder.getOrCreate()\n",
    "# # Load JDBC data to Spark dataframe\n",
    "# dbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\n",
    "# if (dataSet['query']):\n",
    "#     dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\n",
    "# df83 = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\", dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\n",
    "# df83.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the previous cell in last line there would be show() command for the dataset that created. In the next cell change the name of the dataset to cmergedDf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment next line. Use the dataframe name used in the last line of previous cell (for the show() command) instead of df81. Then execute this cell.\n",
    "#cmergedDf = df82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With this data we can create a Binary Classification model that can predict whether a person is a top KoL or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- LONGDISTANCE: string (nullable = true)\n",
      " |-- INTERNATIONAL: string (nullable = true)\n",
      " |-- LOCAL: string (nullable = true)\n",
      " |-- DROPPED: string (nullable = true)\n",
      " |-- PAYMETHOD: string (nullable = true)\n",
      " |-- LOCALBILLTYPE: string (nullable = true)\n",
      " |-- LONGDISTANCEBILLTYPE: string (nullable = true)\n",
      " |-- USAGE: string (nullable = true)\n",
      " |-- RATEPLAN: string (nullable = true)\n",
      " |-- CHURN: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- CHILDREN: string (nullable = true)\n",
      " |-- ESTINCOME: string (nullable = true)\n",
      " |-- CAROWNER: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmergedDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In next 2 lines we create a distributed temporary table/view out of the Dataset so that we can access the data easily using SQL syntax (through use of Spark SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmergedDf.createOrReplaceTempView(\"mergedt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf = spark.sql(\"select CHURN, GENDER, STATUS, CAROWNER, PAYMETHOD, LOCALBILLTYPE, LONGDISTANCEBILLTYPE, \"\\\n",
    "                   \"ID, cast(CHILDREN as integer) as CHILDREN,\"\\\n",
    "                   \"cast(ESTINCOME as double) as ESTINCOME, cast(round(AGE,0) as double) as AGE, cast(DROPPED as integer) as DROPPED,\"\\\n",
    "                   \"cast(RATEPLAN as integer) as RATEPLAN, cast(ID as String) as str_id, \"\\\n",
    "                   \"cast(longdistance as double) as LONGDISTANCE, \"\\\n",
    "                   \"cast(international as double) as INTERNATIONAL, cast(local as double) as LOCAL, cast(usage as double) as USAGE, \"\\\n",
    "                   \"case when AGE < 1 then 'INFANT' when AGE < 18 then 'Child' else 'Adult' End as PHASE from mergedt\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1415"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For developing the Binary Classification Model we want to use Spark ML Lib. Spark MLLib has implementations of various machine learning algorithms that can help creating model on Distributed datasets. Using Spark MLLib, one can create a model using dataset as big as needed for developing a real life Machine Learning model as long as the data is available in the Distributed dataset we created in previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we import components from Spark MLLib for developing the model using the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are importing Pipeline, as that helps building the model development steps as pipeline and the same can be applied to training dataset easily. Additionally we are also importing the VectorAssembler that helps in asembling the attributes (the ones we want to use as features of the model) as Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare string variables so that they can be used by the decision tree algorithm\n",
    "# StringIndexer encodes a string column of labels to a column of label indices\n",
    "SI1 = StringIndexer(inputCol='GENDER', outputCol='GenderEncoded')\n",
    "SI2 = StringIndexer(inputCol='STATUS',outputCol='StatusEncoded')\n",
    "SI3 = StringIndexer(inputCol='CAROWNER',outputCol='CarOwnerEncoded')\n",
    "SI4 = StringIndexer(inputCol='PAYMETHOD',outputCol='PaymethodEncoded')\n",
    "SI5 = StringIndexer(inputCol='LOCALBILLTYPE',outputCol='LocalBilltypeEncoded')\n",
    "SI6 = StringIndexer(inputCol='LONGDISTANCEBILLTYPE',outputCol='LongDistanceBilltypeEncoded')\n",
    "SI7 = StringIndexer(inputCol='PHASE',outputCol='PhaseEncoded')\n",
    "S18 = StringIndexer(inputCol='CHURN', outputCol='label')\n",
    "\n",
    "# Pipelines API requires that input variables are passed in  a vector\n",
    "assembler = VectorAssembler(inputCols=[\"GenderEncoded\", \"StatusEncoded\", \"CarOwnerEncoded\", \"PaymethodEncoded\", \"LocalBilltypeEncoded\", \\\n",
    "                                       \"LongDistanceBilltypeEncoded\", \"PhaseEncoded\", \"CHILDREN\", \"ESTINCOME\", \"AGE\", \"LONGDISTANCE\", \"INTERNATIONAL\", \"LOCAL\",\\\n",
    "                                      \"DROPPED\",\"USAGE\",\"RATEPLAN\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer = S18.fit(custDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the Random Forest algorithm for Binary Classification. One could also use another supervised learning algorithm such as Binary Treee or XG Boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the algorithm, take the default settings\n",
    "rf=RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we define a pipeline with 2 stages. In first stage the features are assembled and in the next stage they are used to create a model using the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SI1,SI2,SI3,SI4,SI5,SI6,SI7,labelIndexer, assembler, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CHURN: string, GENDER: string, STATUS: string, CAROWNER: string, PAYMETHOD: string, LOCALBILLTYPE: string, LONGDISTANCEBILLTYPE: string, ID: string, CHILDREN: int, ESTINCOME: double, AGE: double, DROPPED: int, RATEPLAN: int, str_id: string, LONGDISTANCE: double, INTERNATIONAL: double, LOCAL: double, USAGE: double, PHASE: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and test datasets\n",
    "train, test = custDf.randomSplit([0.7,0.3], seed=6)\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In next step the Model gets created when we apply the pipeline on the Training dataset. It returns the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we test the model by using the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CHURN: string, GENDER: string, STATUS: string, CAROWNER: string, PAYMETHOD: string, LOCALBILLTYPE: string, LONGDISTANCEBILLTYPE: string, ID: string, CHILDREN: int, ESTINCOME: double, AGE: double, DROPPED: int, RATEPLAN: int, str_id: string, LONGDISTANCE: double, INTERNATIONAL: double, LOCAL: double, USAGE: double, PHASE: string, GenderEncoded: double, StatusEncoded: double, CarOwnerEncoded: double, PaymethodEncoded: double, LocalBilltypeEncoded: double, LongDistanceBilltypeEncoded: double, PhaseEncoded: double, label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double, predictedLabel: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CHURN</th>\n",
       "      <th>label</th>\n",
       "      <th>predictedLabel</th>\n",
       "      <th>prediction</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2507</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8653451539990605, 0.13465484600093944]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3414</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8127938153614898, 0.1872061846385103]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>587</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8439980760753223, 0.15600192392467777]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1872</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.7714502461131859, 0.22854975388681412]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>848</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.8088322794224062, 0.19116772057759382]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1287</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.9253756926443769, 0.07462430735562309]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID CHURN  label predictedLabel  prediction  \\\n",
       "0  2507     F    0.0              F         0.0   \n",
       "1  3414     F    0.0              F         0.0   \n",
       "2   587     F    0.0              F         0.0   \n",
       "3  1872     F    0.0              F         0.0   \n",
       "4   848     F    0.0              F         0.0   \n",
       "5  1287     F    0.0              F         0.0   \n",
       "\n",
       "                                 probability  \n",
       "0  [0.8653451539990605, 0.13465484600093944]  \n",
       "1   [0.8127938153614898, 0.1872061846385103]  \n",
       "2  [0.8439980760753223, 0.15600192392467777]  \n",
       "3  [0.7714502461131859, 0.22854975388681412]  \n",
       "4  [0.8088322794224062, 0.19116772057759382]  \n",
       "5  [0.9253756926443769, 0.07462430735562309]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.transform(test)\n",
    "results=results.select(results[\"ID\"],results[\"CHURN\"],results[\"label\"],results[\"predictedLabel\"],results[\"prediction\"],results[\"probability\"])\n",
    "results.toPandas().head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision model = 0.89.\n"
     ]
    }
   ],
   "source": [
    "print('Precision model = {:.2f}.'.format(results.filter(results.label == results.prediction).count() / float(results.count())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we import  the BinaryClassificationEvaluator object from Spark ML Lib for evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We evaluate the model using the area under the Receiver Operating Characteristic (ROC) curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC curve = 0.88.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "print('Area under ROC curve = {:.2f}.'.format(evaluator.evaluate(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we save the Model using MMD infrastructure of Cloud Pak For Da. Please note the URL that is crated after the model is saved. Please use your name as a prefix to model's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Telco Churn Model 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/1001/DSX_Projects/custchurn/models/Telco Churn Model 3/2',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python36/spark-2.3/custchurn/Telco%20Churn%20Model%203/2'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save(name = model_name, model = model, algorithm_type = \"Classification\", test_data = test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save training data as it would be needed by Open Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcuri = \"jdbc:db2://10.187.215.33:31959/BLUDB\"\n",
    "\n",
    "properties = {\n",
    "    \"user\": \"user999\",\n",
    "    \"password\": \"bGSgd%77k7VZ1**@\",\n",
    "    \"driver\": \"com.ibm.db2.jcc.DB2Driver\",\n",
    "    \"sslConnection\":\"false\"\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"modeltrn2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CHURN: string (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- CAROWNER: string (nullable = true)\n",
      " |-- PAYMETHOD: string (nullable = true)\n",
      " |-- LOCALBILLTYPE: string (nullable = true)\n",
      " |-- LONGDISTANCEBILLTYPE: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- CHILDREN: integer (nullable = true)\n",
      " |-- ESTINCOME: double (nullable = true)\n",
      " |-- AGE: double (nullable = true)\n",
      " |-- DROPPED: integer (nullable = true)\n",
      " |-- RATEPLAN: integer (nullable = true)\n",
      " |-- str_id: string (nullable = true)\n",
      " |-- LONGDISTANCE: double (nullable = true)\n",
      " |-- INTERNATIONAL: double (nullable = true)\n",
      " |-- LOCAL: double (nullable = true)\n",
      " |-- USAGE: double (nullable = true)\n",
      " |-- PHASE: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf.write.jdbc(url=jdbcuri, table=TABLE_NAME, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next few cells need to be run if you want to directly deploy the model in WML. Please use your name as prefix to the name of the Model before storing for uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "2019-09-19 04:30:39,088 - watson_machine_learning_client.wml_client_error - WARNING - Unexpected type of 'details', expected: model_details_type, actual: 'unknown_type'.\n"
     ]
    }
   ],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "    \"url\": \"<the url of cp4d without the port>\"\",\n",
    "    \"username\": \"<your user id with administrator permission>\",\n",
    "    \"password\": \">your password\",\n",
    "    \"instance_id\": \"icp\"               \n",
    "}\n",
    "\n",
    "wml_client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_props = {\n",
    "    wml_client.repository.ModelMetaNames.NAME: \"churn-model-in-cp4d-wml\",\n",
    "    wml_client.repository.ModelMetaNames.EVALUATION_METHOD: \"binary\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_model_details = wml_client.repository.store_model(model=model, meta_props=model_props, training_data=train, pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#published_model_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  ------------------------  ------------------------  -----------------\n",
      "GUID                                  NAME                      CREATED                   FRAMEWORK\n",
      "89b02fc5-ecff-4c57-b3e6-cf437243a94f  churn-model-in-cp4d-wml   2019-09-19T04:27:51.086Z  mllib-2.3\n",
      "461e6896-2d71-4251-914b-8ee8b62c68e5  churn-model-in-cp4d-wml   2019-09-17T19:01:04.539Z  mllib-2.3\n",
      "49fa0aa9-30d3-459e-a840-1cc8242b512b  GermanCreditRiskModelICP  2019-09-17T16:32:02.642Z  mllib-2.3\n",
      "75d7afec-7148-45f3-ad67-96ea6fb45680  HouseCreditRiskModelICP   2019-09-17T15:58:17.452Z  scikit-learn-0.19\n",
      "64c9f2c0-1374-42a6-ae08-53e32ddc0a29  GolfModelICP              2019-09-17T15:51:35.223Z  scikit-learn-0.19\n",
      "2ddc52dd-fbdd-4085-bcc0-e42c6a6fde86  DrugSelectionModelICP     2019-09-17T15:31:32.637Z  mllib-2.3\n",
      "------------------------------------  ------------------------  ------------------------  -----------------\n"
     ]
    }
   ],
   "source": [
    "wml_client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
